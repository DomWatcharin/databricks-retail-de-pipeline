from pyspark.sql import functions as F

CATALOG = "retail"
SCHEMA  = "core"

spark.sql(f"USE CATALOG {CATALOG}")
spark.sql(f"USE {SCHEMA}")

def fq(table: str) -> str:
    return f"{CATALOG}.{SCHEMA}.{table}"

def table_exists(table: str) -> bool:
    return spark.catalog.tableExists(fq(table))

def assert_exists(table: str):
    if not table_exists(table):
        raise AssertionError(f"Table not found: {fq(table)}.")

def assert_rowcount_positive(table: str):
    cnt = spark.table(fq(table)).limit(1).count()
    assert cnt > 0, f"{fq(table)} has 0 rows"

def assert_nonnull(table: str, col: str):
    nulls = spark.table(fq(table)).filter(F.col(col).isNull()).limit(1).count()
    assert nulls == 0, f"{fq(table)}.{col} contains NULLs"

# Checks
assert_exists("fact_sales")
assert_rowcount_positive("fact_sales")
assert_nonnull("fact_sales", "event_id")
assert_nonnull("fact_sales", "event_ts")

assert_exists("gm_daily_store_product_sales")
assert_rowcount_positive("gm_daily_store_product_sales")
assert_nonnull("gm_daily_store_product_sales", "revenue")

print("âœ… Basic data quality checks passed.")
